{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcb9d675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functional model created and weights copied!\n",
      "Model1c1 created successfully!\n",
      "Model1c2 created successfully!\n",
      "Model1c3 created successfully!\n",
      "Model1c4 created successfully!\n",
      "All branch models created successfully!\n",
      "\n",
      "Model1c1 input shape: (None, 150, 1)\n",
      "Model1c1 output shape: (None, 1)\n",
      "Model1c2 input shape: (None, 150, 1)\n",
      "Model1c2 output shape: (None, 1)\n",
      "Model1c3 input shape: (None, 150, 1)\n",
      "Model1c3 output shape: (None, 1)\n",
      "Model1c4 input shape: (None, 150, 1)\n",
      "Model1c4 output shape: (None, 1)\n"
     ]
    }
   ],
   "source": [
    "#model-1 represent = CNN-1 (Architecture-1)\n",
    "#This code is creating multiple classifier models that branch off from your main CNN1 model (model1) at different depths.\n",
    "#Instead of having just one classifier that uses all layers, you're creating 4 different classifiers+1-main-CNN (4+1=5 classifie))\n",
    "#that make predictions using features from different stages of your CNN.\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dropout, Flatten, Dense, Conv1D, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create input layer explicitly\n",
    "\n",
    "input_layer = Input(shape=input_shape_1)\n",
    "\n",
    "# Recreate model1's architecture using functional API so we can access intermediate layer outputs.\n",
    "# Based on your original model1 architecture:\n",
    "x = Conv1D(64, kernel_size=3, strides=1, padding='same', activation='relu')(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "layer2_output = Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu')(x)  # This will be layer 2\n",
    "x = BatchNormalization()(layer2_output)\n",
    "layer4_output = Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(x)  # This will be layer 4\n",
    "layer5_output = Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(layer4_output)  # Layer 5\n",
    "layer6_output = Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(layer5_output)\n",
    "layer7_output = BatchNormalization()(layer6_output)  # This will be layer 7\n",
    "x = Dropout(0.5)(layer7_output)\n",
    "x = Flatten()(x)\n",
    "x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "x = Dropout(0.4)(x)\n",
    "main_output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Create the main model (equivalent to your model1)\n",
    "model1_functional = Model(inputs=input_layer, outputs=main_output)\n",
    "model1_functional.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Copying the learned weights from your original trained model1 to this new functional version, so it has the same knowledge.\n",
    "for i, layer in enumerate(model1_functional.layers[1:], 1):  # Skip input layer\n",
    "    if i <= len(model1.layers) and model1.layers[i-1].get_weights():  # If layer has weights\n",
    "        layer.set_weights(model1.layers[i-1].get_weights())\n",
    "\n",
    "print(\"Functional model created and weights copied!\")\n",
    "\n",
    "# Common callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=25)\n",
    "]\n",
    "\n",
    "# Now create branch models from the functional model\n",
    "# Branch Model 1 - from layer 2 equivalent + Add new classifier heads (Dense + sigmoid) that need to be trained\n",
    "x1 = Dropout(0.5)(layer2_output)\n",
    "x1 = Flatten()(x1)\n",
    "x1 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x1)\n",
    "x1 = Dropout(0.5)(x1)\n",
    "branch_output1 = Dense(1, activation='sigmoid')(x1)\n",
    "model1c1 = Model(inputs=input_layer, outputs=branch_output1)\n",
    "model1c1.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model1c1 created successfully!\")\n",
    "\n",
    "# Branch Model 2 - from layer 4 equivalent + Add new classifier heads (Dense + sigmoid) that need to be trained\n",
    "x2 = Dropout(0.5)(layer4_output)\n",
    "x2 = Flatten()(x2)\n",
    "x2 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x2)\n",
    "x2 = Dropout(0.5)(x2)\n",
    "branch_output2 = Dense(1, activation='sigmoid')(x2)\n",
    "model1c2 = Model(inputs=input_layer, outputs=branch_output2)\n",
    "model1c2.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model1c2 created successfully!\")\n",
    "\n",
    "# Branch Model 3 - from layer 5 equivalent  + Add new classifier heads (Dense + sigmoid) that need to be trained\n",
    "x3 = Dropout(0.5)(layer5_output)\n",
    "x3 = Flatten()(x3)\n",
    "x3 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x3)\n",
    "x3 = Dropout(0.5)(x3)\n",
    "branch_output3 = Dense(1, activation='sigmoid')(x3)\n",
    "model1c3 = Model(inputs=input_layer, outputs=branch_output3)\n",
    "model1c3.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model1c3 created successfully!\")\n",
    "\n",
    "# Branch Model 4 - from layer 6 equivalent + Add new classifier heads (Dense + sigmoid) that need to be trained\n",
    "x4 = Dropout(0.5)(layer6_output)\n",
    "x4 = Flatten()(x4)\n",
    "x4 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x4)\n",
    "x4 = Dropout(0.5)(x4)\n",
    "branch_output4 = Dense(1, activation='sigmoid')(x4)\n",
    "model1c4 = Model(inputs=input_layer, outputs=branch_output4)\n",
    "model1c4.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model1c4 created successfully!\")\n",
    "print(\"All branch models created successfully!\")\n",
    "\n",
    "# Verify the models work\n",
    "print(f\"\\nModel1c1 input shape: {model1c1.input_shape}\")\n",
    "print(f\"Model1c1 output shape: {model1c1.output_shape}\")\n",
    "print(f\"Model1c2 input shape: {model1c2.input_shape}\")\n",
    "print(f\"Model1c2 output shape: {model1c2.output_shape}\")\n",
    "print(f\"Model1c3 input shape: {model1c3.input_shape}\")\n",
    "print(f\"Model1c3 output shape: {model1c3.output_shape}\")\n",
    "print(f\"Model1c4 input shape: {model1c4.input_shape}\")\n",
    "print(f\"Model1c4 output shape: {model1c4.output_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74654b05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a717190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07720765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model2 functional model created and weights copied!\n",
      "Model2c1 created successfully!\n",
      "Model2c2 created successfully!\n",
      "Model2c3 created successfully!\n",
      "Model2c4 created successfully!\n",
      "Model2c5 created successfully!\n",
      "Model2c6 created successfully!\n",
      "All model2 branch models created successfully!\n",
      "\n",
      "Model2c1 input shape: (None, 150, 1)\n",
      "Model2c1 output shape: (None, 1)\n",
      "Model2c2 input shape: (None, 150, 1)\n",
      "Model2c2 output shape: (None, 1)\n",
      "Model2c3 input shape: (None, 150, 1)\n",
      "Model2c3 output shape: (None, 1)\n",
      "Model2c4 input shape: (None, 150, 1)\n",
      "Model2c4 output shape: (None, 1)\n",
      "Model2c5 input shape: (None, 150, 1)\n",
      "Model2c5 output shape: (None, 1)\n",
      "Model2c6 input shape: (None, 150, 1)\n",
      "Model2c6 output shape: (None, 1)\n"
     ]
    }
   ],
   "source": [
    "#model-2 represent = CNN-2 (Architecture-2)\n",
    "#This code is creating multiple classifier models that branch off from your main CNN2 model (model2) at different depths.\n",
    "#Instead of having just one classifier that uses all layers, you're creating 6 different classifiers+1-main-CNN (6+1=7 classifie))\n",
    "#that make predictions using features from different stages of your CNN.\n",
    "# Create input layer explicitly for model2\n",
    "input_layer = Input(shape=input_shape_1)\n",
    "\n",
    "# Recreate model2's architecture using functional API\n",
    "# Keep first Conv1D(16) block, ignore the last Conv1D(128) block\n",
    "\n",
    "# Layer 1-3: Conv1D + BatchNorm + MaxPooling\n",
    "layer1_output = Conv1D(16, kernel_size=3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(input_layer)\n",
    "x = BatchNormalization()(layer1_output)\n",
    "layer3_output = MaxPooling1D(pool_size=1)(x)  # This will be layer 3 (first branch point)\n",
    "\n",
    "# Layer 4-6: Conv1D + BatchNorm + MaxPooling\n",
    "layer4_output = Conv1D(32, kernel_size=3, strides=1, padding='same', activation='relu')(layer3_output)\n",
    "x = BatchNormalization()(layer4_output)\n",
    "layer6_output = MaxPooling1D(pool_size=1)(x)  # This will be layer 6 (second branch point)\n",
    "\n",
    "# Layer 7-9: Conv1D + BatchNorm + MaxPooling\n",
    "layer7_output = Conv1D(32, kernel_size=3, strides=1, padding='same', activation='relu')(layer6_output)\n",
    "x = BatchNormalization()(layer7_output)\n",
    "layer9_output = MaxPooling1D(pool_size=1)(x)  # This will be layer 9 (third branch point)\n",
    "\n",
    "# Layer 10-12: Conv1D + BatchNorm + MaxPooling\n",
    "layer10_output = Conv1D(64, kernel_size=3, strides=1, padding='same', activation='relu')(layer9_output)\n",
    "x = BatchNormalization()(layer10_output)\n",
    "layer12_output = MaxPooling1D(pool_size=1)(x)  # This will be layer 12 (fourth branch point)\n",
    "\n",
    "# Layer 13-15: Conv1D + BatchNorm + MaxPooling\n",
    "layer13_output = Conv1D(64, kernel_size=3, strides=1, padding='same', activation='relu')(layer12_output)\n",
    "x = BatchNormalization()(layer13_output)\n",
    "layer15_output = MaxPooling1D(pool_size=1)(x)  # This will be layer 15 (fifth branch point)\n",
    "\n",
    "# Layer 16-18: Conv1D + BatchNorm + MaxPooling\n",
    "layer16_output = Conv1D(128, kernel_size=3, strides=1, padding='same', activation='relu')(layer15_output)\n",
    "x = BatchNormalization()(layer16_output)\n",
    "layer18_output = MaxPooling1D(pool_size=1)(x)  # This will be layer 18 (sixth branch point)\n",
    "\n",
    "# Final layers (ignore the last Conv1D(128) block)\n",
    "x = Dropout(0.3)(layer18_output)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "main_output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Create the main model (equivalent to your model2)\n",
    "model2_functional = Model(inputs=input_layer, outputs=main_output)\n",
    "model2_functional.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Copy weights from your original model2 to the new functional model\n",
    "for i, layer in enumerate(model2_functional.layers[1:], 1):  # Skip input layer\n",
    "    if i <= len(model2.layers) and model2.layers[i-1].get_weights():  # If layer has weights\n",
    "        try:\n",
    "            layer.set_weights(model2.layers[i-1].get_weights())\n",
    "        except ValueError:\n",
    "            # Skip layers that don't match (Dropout, Flatten, Dense layers with different shapes)\n",
    "            continue\n",
    "\n",
    "print(\"Model2 functional model created and weights copied!\")\n",
    "\n",
    "# Common callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=25)\n",
    "]\n",
    "\n",
    "# Branch Model 1 - from layer 3 equivalent (after first Conv1D block)\n",
    "x1 = Dropout(0.3)(layer1_output)\n",
    "x1 = Flatten()(x1)\n",
    "x1 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x1)\n",
    "x1 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Dropout(0.3)(x1)\n",
    "x1 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x1)\n",
    "x1 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x1)\n",
    "branch_output1 = Dense(1, activation='sigmoid')(x1)\n",
    "model2c1 = Model(inputs=input_layer, outputs=branch_output1)\n",
    "model2c1.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model2c1 created successfully!\")\n",
    "\n",
    "# Branch Model 2 - from layer 6 equivalent\n",
    "x2 = Dropout(0.3)(layer4_output)\n",
    "x2 = Flatten()(x2)\n",
    "x2 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x2)\n",
    "x2 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Dropout(0.3)(x2)\n",
    "x2 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x2)\n",
    "x2 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x2)\n",
    "branch_output2 = Dense(1, activation='sigmoid')(x2)\n",
    "model2c2 = Model(inputs=input_layer, outputs=branch_output2)\n",
    "model2c2.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model2c2 created successfully!\")\n",
    "\n",
    "# Branch Model 3 - from layer 9 equivalent\n",
    "x3 = Dropout(0.3)(layer7_output)\n",
    "x3 = Flatten()(x3)\n",
    "x3 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x3)\n",
    "x3 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x3)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = Dropout(0.3)(x3)\n",
    "x3 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x3)\n",
    "x3 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x3)\n",
    "branch_output3 = Dense(1, activation='sigmoid')(x3)\n",
    "model2c3 = Model(inputs=input_layer, outputs=branch_output3)\n",
    "model2c3.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model2c3 created successfully!\")\n",
    "\n",
    "# Branch Model 4 - from layer 12 equivalent\n",
    "x4 = Dropout(0.3)(layer10_output)\n",
    "x4 = Flatten()(x4)\n",
    "x4 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x4)\n",
    "x4 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x4)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Dropout(0.3)(x4)\n",
    "x4 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x4)\n",
    "x4 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x4)\n",
    "branch_output4 = Dense(1, activation='sigmoid')(x4)\n",
    "model2c4 = Model(inputs=input_layer, outputs=branch_output4)\n",
    "model2c4.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model2c4 created successfully!\")\n",
    "\n",
    "# Branch Model 5 - from layer 15 equivalent\n",
    "x5 = Dropout(0.3)(layer13_output)\n",
    "x5 = Flatten()(x5)\n",
    "x5 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x5)\n",
    "x5 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x5)\n",
    "x5 = BatchNormalization()(x5)\n",
    "x5 = Dropout(0.3)(x5)\n",
    "x5 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x5)\n",
    "x5 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x5)\n",
    "branch_output5 = Dense(1, activation='sigmoid')(x5)\n",
    "model2c5 = Model(inputs=input_layer, outputs=branch_output5)\n",
    "model2c5.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model2c5 created successfully!\")\n",
    "\n",
    "# Branch Model 6 - from layer 18 equivalent\n",
    "x6 = Dropout(0.3)(layer16_output)\n",
    "x6 = Flatten()(x6)\n",
    "x6 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x6)\n",
    "x6 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x6)\n",
    "x6 = BatchNormalization()(x6)\n",
    "x6 = Dropout(0.3)(x6)\n",
    "x6 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x6)\n",
    "x6 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x6)\n",
    "branch_output6 = Dense(1, activation='sigmoid')(x6)\n",
    "model2c6 = Model(inputs=input_layer, outputs=branch_output6)\n",
    "model2c6.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model2c6 created successfully!\")\n",
    "print(\"All model2 branch models created successfully!\")\n",
    "\n",
    "# Verify the models work\n",
    "print(f\"\\nModel2c1 input shape: {model2c1.input_shape}\")\n",
    "print(f\"Model2c1 output shape: {model2c1.output_shape}\")\n",
    "print(f\"Model2c2 input shape: {model2c2.input_shape}\")\n",
    "print(f\"Model2c2 output shape: {model2c2.output_shape}\")\n",
    "print(f\"Model2c3 input shape: {model2c3.input_shape}\")\n",
    "print(f\"Model2c3 output shape: {model2c3.output_shape}\")\n",
    "print(f\"Model2c4 input shape: {model2c4.input_shape}\")\n",
    "print(f\"Model2c4 output shape: {model2c4.output_shape}\")\n",
    "print(f\"Model2c5 input shape: {model2c5.input_shape}\")\n",
    "print(f\"Model2c5 output shape: {model2c5.output_shape}\")\n",
    "print(f\"Model2c6 input shape: {model2c6.input_shape}\")\n",
    "print(f\"Model2c6 output shape: {model2c6.output_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f66df34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3c230d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model3 functional model created and weights copied!\n",
      "Model3c1 created successfully!\n",
      "Model3c2 created successfully!\n",
      "Model3c3 created successfully!\n",
      "Model3c4 created successfully!\n",
      "Model3c5 created successfully!\n",
      "All model3 branch models created successfully!\n",
      "\n",
      "Model3c1 input shape: (None, 84, 1)\n",
      "Model3c1 output shape: (None, 1)\n",
      "Model3c2 input shape: (None, 84, 1)\n",
      "Model3c2 output shape: (None, 1)\n",
      "Model3c3 input shape: (None, 84, 1)\n",
      "Model3c3 output shape: (None, 1)\n",
      "Model3c4 input shape: (None, 84, 1)\n",
      "Model3c4 output shape: (None, 1)\n"
     ]
    }
   ],
   "source": [
    "#model-3 represent = CNN-3 (Architecture-1)\n",
    "\n",
    "#This code is creating multiple classifier models that branch off from your main CNN3 model (model3) at different depths.\n",
    "#Instead of having just one classifier that uses all layers, you're creating 4 different classifiers(+1 main CNN(4+1=5 classifie))\n",
    "#that make predictions using features from different stages of your CNN.\n",
    "\n",
    "\n",
    "# Create input layer explicitly for model3\n",
    "input_layer = Input(shape=input_shape_2)\n",
    "\n",
    "# Recreate model3's architecture using functional API so we can access intermediate layer outputs.\n",
    "# Based on your original model3 architecture:\n",
    "x = Conv1D(64, kernel_size=3, strides=1, padding='same', activation='relu')(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "layer2_output = Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu')(x)  # This will be layer 2\n",
    "x = BatchNormalization()(layer2_output)\n",
    "layer4_output = Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(x)  # This will be layer 4\n",
    "layer5_output = Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(layer4_output)  # Layer 5\n",
    "layer6_output = Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(layer5_output)  # Layer 6\n",
    "x = BatchNormalization()(layer6_output)\n",
    "layer8_output = Dropout(0.5)(x)  # This will be layer 8\n",
    "x = Flatten()(layer8_output)\n",
    "x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "x = Dropout(0.4)(x)\n",
    "main_output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Create the main model (equivalent to your model3)\n",
    "model3_functional = Model(inputs=input_layer, outputs=main_output)\n",
    "model3_functional.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Copying the learned weights from your original trained model3 to this new functional version, so it has the same knowledge.\n",
    "for i, layer in enumerate(model3_functional.layers[1:], 1):  # Skip input layer\n",
    "    if i <= len(model3.layers) and model3.layers[i-1].get_weights():  # If layer has weights\n",
    "        layer.set_weights(model3.layers[i-1].get_weights())\n",
    "\n",
    "print(\"Model3 functional model created and weights copied!\")\n",
    "\n",
    "# Common callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=25)\n",
    "]\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# Branch Model 1 - from layer 2 equivalent + Add new classifier heads (Dense + sigmoid) that need to be trained\n",
    "x1 = Dropout(0.5)(layer2_output)\n",
    "x1 = Flatten()(x1)\n",
    "x1 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x1)\n",
    "x1 = Dropout(0.5)(x1)\n",
    "branch_output1 = Dense(1, activation='sigmoid')(x1)\n",
    "model3c1 = Model(inputs=input_layer, outputs=branch_output1)\n",
    "model3c1.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model3c1 created successfully!\")\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Branch Model 2 - from layer 4 equivalent + Add new classifier heads (Dense + sigmoid) that need to be trained\n",
    "x2 = Dropout(0.5)(layer4_output)\n",
    "x2 = Flatten()(x2)\n",
    "x2 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x2)\n",
    "x2 = Dropout(0.5)(x2)\n",
    "branch_output2 = Dense(1, activation='sigmoid')(x2)\n",
    "model3c2 = Model(inputs=input_layer, outputs=branch_output2)\n",
    "model3c2.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model3c2 created successfully!\")\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Branch Model 3 - from layer 5 equivalent + Add new classifier heads (Dense + sigmoid) that need to be trained\n",
    "x3 = Dropout(0.5)(layer5_output)\n",
    "x3 = Flatten()(x3)\n",
    "x3 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x3)\n",
    "x3 = Dropout(0.5)(x3)\n",
    "branch_output3 = Dense(1, activation='sigmoid')(x3)\n",
    "model3c3 = Model(inputs=input_layer, outputs=branch_output3)\n",
    "model3c3.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model3c3 created successfully!\")\n",
    "\n",
    "#--------------------------------------------------------------------------------------------\n",
    "\n",
    "# Branch Model 4 - from layer 6 equivalent  + Add new classifier heads (Dense + sigmoid) that need to be trained\n",
    "x4 = Dropout(0.5)(layer6_output)\n",
    "x4 = Flatten()(x4)\n",
    "x4 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x4)\n",
    "x4 = Dropout(0.5)(x4)\n",
    "branch_output4 = Dense(1, activation='sigmoid')(x4)\n",
    "model3c4 = Model(inputs=input_layer, outputs=branch_output4)\n",
    "model3c4.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model3c4 created successfully!\")\n",
    "print(\"Model3c5 created successfully!\")\n",
    "print(\"All model3 branch models created successfully!\")\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "# Verify the models work\n",
    "print(f\"\\nModel3c1 input shape: {model3c1.input_shape}\")\n",
    "print(f\"Model3c1 output shape: {model3c1.output_shape}\")\n",
    "print(f\"Model3c2 input shape: {model3c2.input_shape}\")\n",
    "print(f\"Model3c2 output shape: {model3c2.output_shape}\")\n",
    "print(f\"Model3c3 input shape: {model3c3.input_shape}\")\n",
    "print(f\"Model3c3 output shape: {model3c3.output_shape}\")\n",
    "print(f\"Model3c4 input shape: {model3c4.input_shape}\")\n",
    "print(f\"Model3c4 output shape: {model3c4.output_shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f97d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee105b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model4 functional model created and weights copied!\n",
      "Model4c1 created successfully!\n",
      "Model4c2 created successfully!\n",
      "Model4c3 created successfully!\n",
      "Model4c4 created successfully!\n",
      "Model4c5 created successfully!\n",
      "Model4c6 created successfully!\n",
      "All model4 branch models created successfully!\n",
      "\n",
      "Model4c1 input shape: (None, 84, 1)\n",
      "Model4c1 output shape: (None, 1)\n",
      "Model4c2 input shape: (None, 84, 1)\n",
      "Model4c2 output shape: (None, 1)\n",
      "Model4c3 input shape: (None, 84, 1)\n",
      "Model4c3 output shape: (None, 1)\n",
      "Model4c4 input shape: (None, 84, 1)\n",
      "Model4c4 output shape: (None, 1)\n",
      "Model4c5 input shape: (None, 84, 1)\n",
      "Model4c5 output shape: (None, 1)\n",
      "Model4c6 input shape: (None, 84, 1)\n",
      "Model4c6 output shape: (None, 1)\n"
     ]
    }
   ],
   "source": [
    "#model-4 represent = CNN-4 (Architecture-2)\n",
    "#This code is creating multiple classifier models that branch off from your main CNN4 model (model4) at different depths.\n",
    "#Instead of having just one classifier that uses all layers, you're creating 6 different classifiers+1-main-CNN (6+1=7 classifie))\n",
    "#that make predictions using features from different stages of your CNN.\n",
    "\n",
    "# Create input layer explicitly for model4\n",
    "input_layer = Input(shape=input_shape_2)\n",
    "\n",
    "# Recreate model4's architecture using functional API\n",
    "# Keep first Conv1D(16) block, ignore the last Conv1D(128) block (same as model2 fix)\n",
    "\n",
    "# Layer 1-3: Conv1D + BatchNorm + MaxPooling\n",
    "layer1_output = Conv1D(16, kernel_size=3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(input_layer)\n",
    "x = BatchNormalization()(layer1_output)\n",
    "layer3_output = MaxPooling1D(pool_size=1)(x)  # This will be layer 3 (first branch point)\n",
    "\n",
    "# Layer 4-6: Conv1D + BatchNorm + MaxPooling\n",
    "# FIX 1: Corrected variable name from layer4_outputx to layer4_output\n",
    "layer4_output = Conv1D(32, kernel_size=3, strides=1, padding='same', activation='relu')(layer3_output)\n",
    "x = BatchNormalization()(layer4_output)\n",
    "layer6_output = MaxPooling1D(pool_size=1)(x)  # This will be layer 6 (second branch point)\n",
    "\n",
    "# Layer 7-9: Conv1D + BatchNorm + MaxPooling\n",
    "layer7_output = Conv1D(32, kernel_size=3, strides=1, padding='same', activation='relu')(layer6_output)\n",
    "x = BatchNormalization()(layer7_output)\n",
    "layer9_output = MaxPooling1D(pool_size=1)(x)  # This will be layer 9 (third branch point)\n",
    "\n",
    "# Layer 10-12: Conv1D + BatchNorm + MaxPooling\n",
    "layer10_output = Conv1D(64, kernel_size=3, strides=1, padding='same', activation='relu')(layer9_output)\n",
    "x = BatchNormalization()(layer10_output)\n",
    "layer12_output = MaxPooling1D(pool_size=1)(x)  # This will be layer 12 (fourth branch point)\n",
    "\n",
    "# Layer 13-15: Conv1D + BatchNorm + MaxPooling\n",
    "layer13_output = Conv1D(64, kernel_size=3, strides=1, padding='same', activation='relu')(layer12_output)\n",
    "x = BatchNormalization()(layer13_output)\n",
    "layer15_output = MaxPooling1D(pool_size=1)(x)  # This will be layer 15 (fifth branch point)\n",
    "\n",
    "# Layer 16-18: Conv1D + BatchNorm + MaxPooling\n",
    "layer16_output = Conv1D(128, kernel_size=3, strides=1, padding='same', activation='relu')(layer15_output)\n",
    "# FIX 2: Corrected input to BatchNormalization - should be layer16_output, not layer15_output\n",
    "x = BatchNormalization()(layer16_output)\n",
    "layer18_output = MaxPooling1D(pool_size=1)(x)  # This will be layer 18 (sixth branch point)\n",
    "\n",
    "# Final layers (ignore the last Conv1D(128) block)\n",
    "x = Dropout(0.3)(layer18_output)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "main_output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Create the main model (equivalent to your model4)\n",
    "model4_functional = Model(inputs=input_layer, outputs=main_output)\n",
    "model4_functional.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Copy weights from your original model4 to the new functional model\n",
    "for i, layer in enumerate(model4_functional.layers[1:], 1):  # Skip input layer\n",
    "    if i <= len(model4.layers) and model4.layers[i-1].get_weights():  # If layer has weights\n",
    "        try:\n",
    "            layer.set_weights(model4.layers[i-1].get_weights())\n",
    "        except ValueError:\n",
    "            # Skip layers that don't match (Dropout, Flatten, Dense layers with different shapes)\n",
    "            continue\n",
    "\n",
    "print(\"Model4 functional model created and weights copied!\")\n",
    "\n",
    "# Common callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=25)\n",
    "]\n",
    "\n",
    "# Branch Model 1 - from layer 3 equivalent (after first Conv1D block)\n",
    "x1 = Dropout(0.3)(layer1_output)\n",
    "x1 = Flatten()(x1)\n",
    "x1 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x1)\n",
    "x1 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Dropout(0.3)(x1)\n",
    "x1 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x1)\n",
    "x1 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x1)\n",
    "branch_output1 = Dense(1, activation='sigmoid')(x1)\n",
    "model4c1 = Model(inputs=input_layer, outputs=branch_output1)\n",
    "model4c1.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model4c1 created successfully!\")\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Branch Model 2 - from layer 6 equivalent\n",
    "x2 = Dropout(0.3)(layer4_output)\n",
    "x2 = Flatten()(x2)\n",
    "x2 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x2)\n",
    "x2 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Dropout(0.3)(x2)\n",
    "x2 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x2)\n",
    "x2 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x2)\n",
    "branch_output2 = Dense(1, activation='sigmoid')(x2)\n",
    "model4c2 = Model(inputs=input_layer, outputs=branch_output2)\n",
    "model4c2.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model4c2 created successfully!\")\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Branch Model 3 - from layer 9 equivalent\n",
    "x3 = Dropout(0.3)(layer7_output)\n",
    "x3 = Flatten()(x3)\n",
    "x3 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x3)\n",
    "x3 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x3)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = Dropout(0.3)(x3)\n",
    "x3 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x3)\n",
    "x3 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x3)\n",
    "branch_output3 = Dense(1, activation='sigmoid')(x3)\n",
    "model4c3 = Model(inputs=input_layer, outputs=branch_output3)\n",
    "model4c3.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model4c3 created successfully!\")\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Branch Model 4 - from layer 12 equivalent\n",
    "x4 = Dropout(0.3)(layer10_output)\n",
    "x4 = Flatten()(x4)\n",
    "x4 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x4)\n",
    "x4 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x4)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Dropout(0.3)(x4)\n",
    "x4 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x4)\n",
    "x4 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x4)\n",
    "branch_output4 = Dense(1, activation='sigmoid')(x4)\n",
    "model4c4 = Model(inputs=input_layer, outputs=branch_output4)\n",
    "model4c4.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model4c4 created successfully!\")\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Branch Model 5 - from layer 15 equivalent\n",
    "x5 = Dropout(0.3)(layer13_output)\n",
    "x5 = Flatten()(x5)\n",
    "x5 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x5)\n",
    "x5 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x5)\n",
    "x5 = BatchNormalization()(x5)\n",
    "x5 = Dropout(0.3)(x5)\n",
    "x5 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x5)\n",
    "x5 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x5)\n",
    "branch_output5 = Dense(1, activation='sigmoid')(x5)\n",
    "model4c5 = Model(inputs=input_layer, outputs=branch_output5)\n",
    "# FIX 3: Corrected typo from 'binary_crossentrology' to 'binary_crossentropy'\n",
    "model4c5.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model4c5 created successfully!\")\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Branch Model 6 - from layer 18 equivalent\n",
    "x6 = Dropout(0.3)(layer16_output)\n",
    "x6 = Flatten()(x6)\n",
    "x6 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x6)\n",
    "x6 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x6)\n",
    "x6 = BatchNormalization()(x6)\n",
    "x6 = Dropout(0.3)(x6)\n",
    "x6 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x6)\n",
    "x6 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x6)\n",
    "branch_output6 = Dense(1, activation='sigmoid')(x6)\n",
    "model4c6 = Model(inputs=input_layer, outputs=branch_output6)\n",
    "model4c6.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model4c6 created successfully!\")\n",
    "print(\"All model4 branch models created successfully!\")\n",
    "\n",
    "# Verify the models work\n",
    "print(f\"\\nModel4c1 input shape: {model4c1.input_shape}\")\n",
    "print(f\"Model4c1 output shape: {model4c1.output_shape}\")\n",
    "print(f\"Model4c2 input shape: {model4c2.input_shape}\")\n",
    "print(f\"Model4c2 output shape: {model4c2.output_shape}\")\n",
    "print(f\"Model4c3 input shape: {model4c3.input_shape}\")\n",
    "print(f\"Model4c3 output shape: {model4c3.output_shape}\")\n",
    "print(f\"Model4c4 input shape: {model4c4.input_shape}\")\n",
    "print(f\"Model4c4 output shape: {model4c4.output_shape}\")\n",
    "print(f\"Model4c5 input shape: {model4c5.input_shape}\")\n",
    "print(f\"Model4c5 output shape: {model4c5.output_shape}\")\n",
    "print(f\"Model4c6 input shape: {model4c6.input_shape}\")\n",
    "print(f\"Model4c6 output shape: {model4c6.output_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774a0e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30eb870e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model5 functional model created and weights copied!\n",
      "Model5c1 created successfully!\n",
      "Model5c2 created successfully!\n",
      "Model5c3 created successfully!\n",
      "Model5c4 created successfully!\n",
      "All model5 branch models created successfully!\n",
      "\n",
      "Model5c1 input shape: (None, 420, 1)\n",
      "Model5c1 output shape: (None, 1)\n",
      "Model5c2 input shape: (None, 420, 1)\n",
      "Model5c2 output shape: (None, 1)\n",
      "Model5c3 input shape: (None, 420, 1)\n",
      "Model5c3 output shape: (None, 1)\n",
      "Model5c4 input shape: (None, 420, 1)\n",
      "Model5c4 output shape: (None, 1)\n"
     ]
    }
   ],
   "source": [
    "#model-5 represent = CNN-5 (Architecture-1)\n",
    "#This code is creating multiple classifier models that branch off from your main CNN5 model (model5) at different depths.\n",
    "#Instead of having just one classifier that uses all layers, you're creating 4 different classifiers(+1 main CNN(4+1=5 classifie))\n",
    "#that make predictions using features from different stages of your CNN.\n",
    "from tensorflow.keras.layers import Input, Dropout, Flatten, Dense, Conv1D, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create input layer explicitly for model5\n",
    "input_layer = Input(shape=input_shape_3)\n",
    "\n",
    "# Recreate model3's architecture using functional API so we can access intermediate layer outputs.\n",
    "# Based on your original model5 architecture:\n",
    "x = Conv1D(64, kernel_size=3, strides=1, padding='same', activation='relu')(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "layer2_output = Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu')(x)  # This will be layer 2\n",
    "x = BatchNormalization()(layer2_output)\n",
    "layer4_output = Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(x)  # This will be layer 4\n",
    "layer5_output = Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(layer4_output)  # Layer 5\n",
    "layer6_output = Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu')(layer5_output)  # Layer 6\n",
    "x = BatchNormalization()(layer6_output)\n",
    "layer8_output = Dropout(0.5)(x)  # This will be layer 8\n",
    "x = Flatten()(layer8_output)\n",
    "x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "x = Dropout(0.4)(x)\n",
    "main_output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Create the main model (equivalent to your model5)\n",
    "model5_functional = Model(inputs=input_layer, outputs=main_output)\n",
    "model5_functional.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Copying the learned weights from your original trained model5 to this new functional version, so it has the same knowledge.\n",
    "for i, layer in enumerate(model5_functional.layers[1:], 1):  # Skip input layer\n",
    "    if i <= len(model5.layers) and model5.layers[i-1].get_weights():  # If layer has weights\n",
    "        layer.set_weights(model5.layers[i-1].get_weights())\n",
    "\n",
    "print(\"Model5 functional model created and weights copied!\")\n",
    "\n",
    "# Common callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=25)\n",
    "]\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# Branch Model 1 - from layer 2 equivalent + Add new classifier heads (Dense + sigmoid) that need to be trained\n",
    "x1 = Dropout(0.5)(layer2_output)\n",
    "x1 = Flatten()(x1)\n",
    "x1 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x1)\n",
    "x1 = Dropout(0.5)(x1)\n",
    "branch_output1 = Dense(1, activation='sigmoid')(x1)\n",
    "model5c1 = Model(inputs=input_layer, outputs=branch_output1)\n",
    "model5c1.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model5c1 created successfully!\")\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Branch Model 2 - from layer 4 equivalent + Add new classifier heads (Dense + sigmoid) that need to be trained\n",
    "x2 = Dropout(0.5)(layer4_output)\n",
    "x2 = Flatten()(x2)\n",
    "x2 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x2)\n",
    "x2 = Dropout(0.5)(x2)\n",
    "branch_output2 = Dense(1, activation='sigmoid')(x2)\n",
    "model5c2 = Model(inputs=input_layer, outputs=branch_output2)\n",
    "model5c2.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model5c2 created successfully!\")\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Branch Model 3 - from layer 5 equivalent + Add new classifier heads (Dense + sigmoid) that need to be trained\n",
    "x3 = Dropout(0.5)(layer5_output)\n",
    "x3 = Flatten()(x3)\n",
    "x3 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x3)\n",
    "x3 = Dropout(0.5)(x3)\n",
    "branch_output3 = Dense(1, activation='sigmoid')(x3)\n",
    "model5c3 = Model(inputs=input_layer, outputs=branch_output3)\n",
    "model5c3.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model5c3 created successfully!\")\n",
    "\n",
    "#--------------------------------------------------------------------------------------------\n",
    "\n",
    "# Branch Model 4 - from layer 6 equivalent + Add new classifier heads (Dense + sigmoid) that need to be trained\n",
    "x4 = Dropout(0.5)(layer6_output)\n",
    "x4 = Flatten()(x4)\n",
    "x4 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x4)\n",
    "x4 = Dropout(0.5)(x4)\n",
    "branch_output4 = Dense(1, activation='sigmoid')(x4)\n",
    "model5c4 = Model(inputs=input_layer, outputs=branch_output4)\n",
    "model5c4.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model5c4 created successfully!\")\n",
    "\n",
    "print(\"All model5 branch models created successfully!\")\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "# Verify the models work\n",
    "print(f\"\\nModel5c1 input shape: {model5c1.input_shape}\")\n",
    "print(f\"Model5c1 output shape: {model5c1.output_shape}\")\n",
    "print(f\"Model5c2 input shape: {model5c2.input_shape}\")\n",
    "print(f\"Model5c2 output shape: {model5c2.output_shape}\")\n",
    "print(f\"Model5c3 input shape: {model5c3.input_shape}\")\n",
    "print(f\"Model5c3 output shape: {model5c3.output_shape}\")\n",
    "print(f\"Model5c4 input shape: {model5c4.input_shape}\")\n",
    "print(f\"Model5c4 output shape: {model5c4.output_shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5d0715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7378322d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429a2697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "287f1ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model6 functional model created and weights copied!\n",
      "Model6c1 created successfully!\n",
      "Model6c2 created successfully!\n",
      "Model6c3 created successfully!\n",
      "Model6c4 created successfully!\n",
      "Model6c5 created successfully!\n",
      "Model6c6 created successfully!\n",
      "All model6 branch models created successfully!\n",
      "\n",
      "Model6c1 input shape: (None, 420, 1)\n",
      "Model6c1 output shape: (None, 1)\n",
      "Model6c2 input shape: (None, 420, 1)\n",
      "Model6c2 output shape: (None, 1)\n",
      "Model6c3 input shape: (None, 420, 1)\n",
      "Model6c3 output shape: (None, 1)\n",
      "Model6c4 input shape: (None, 420, 1)\n",
      "Model6c4 output shape: (None, 1)\n",
      "Model6c5 input shape: (None, 420, 1)\n",
      "Model6c5 output shape: (None, 1)\n",
      "Model6c6 input shape: (None, 420, 1)\n",
      "Model6c6 output shape: (None, 1)\n"
     ]
    }
   ],
   "source": [
    "#model-6 represent = CNN-6 (Architecture-2)\n",
    "#This code is creating multiple classifier models that branch off from your main CNN6 model (model6) at different depths.\n",
    "#Instead of having just one classifier that uses all layers, you're creating 6 different classifiers+1-main-CNN (6+1=7 classifie))\n",
    "#that make predictions using features from different stages of your CNN.\n",
    "\n",
    "# Create input layer explicitly for model6\n",
    "input_layer = Input(shape=input_shape_3)\n",
    "\n",
    "# Recreate model6's architecture using functional API\n",
    "# Keep first Conv1D(16) block, ignore the last Conv1D(128) block (same as model2 and model4 fix)\n",
    "\n",
    "# Layer 1-3: Conv1D + BatchNorm + MaxPooling\n",
    "layer1_output = Conv1D(16, kernel_size=3, strides=1, padding='same', activation='relu', kernel_initializer='he_normal')(input_layer)\n",
    "x = BatchNormalization()(layer1_output)\n",
    "layer3_output = MaxPooling1D(pool_size=1)(x)  # This will be layer 3 (first branch point)\n",
    "\n",
    "# Layer 4-6: Conv1D + BatchNorm + MaxPooling\n",
    "layer4_output = Conv1D(32, kernel_size=3, strides=1, padding='same', activation='relu')(layer3_output)\n",
    "x = BatchNormalization()(layer4_output)\n",
    "layer6_output = MaxPooling1D(pool_size=1)(x)  # This will be layer 6 (second branch point)\n",
    "\n",
    "# Layer 7-9: Conv1D + BatchNorm + MaxPooling\n",
    "layer7_output = Conv1D(32, kernel_size=3, strides=1, padding='same', activation='relu')(layer6_output)\n",
    "x = BatchNormalization()(layer7_output)\n",
    "layer9_output = MaxPooling1D(pool_size=1)(x)  # This will be layer 9 (third branch point)\n",
    "\n",
    "# Layer 10-12: Conv1D + BatchNorm + MaxPooling\n",
    "layer10_output = Conv1D(64, kernel_size=3, strides=1, padding='same', activation='relu')(layer9_output)\n",
    "x = BatchNormalization()(layer10_output)\n",
    "layer12_output = MaxPooling1D(pool_size=1)(x)  # This will be layer 12 (fourth branch point)\n",
    "\n",
    "# Layer 13-15: Conv1D + BatchNorm + MaxPooling\n",
    "layer13_output = Conv1D(64, kernel_size=3, strides=1, padding='same', activation='relu')(layer12_output)\n",
    "x = BatchNormalization()(layer13_output)\n",
    "layer15_output = MaxPooling1D(pool_size=1)(x)  # This will be layer 15 (fifth branch point)\n",
    "\n",
    "# Layer 16-18: Conv1D + BatchNorm + MaxPooling\n",
    "layer16_output = Conv1D(128, kernel_size=3, strides=1, padding='same', activation='relu')(layer15_output)\n",
    "x = BatchNormalization()(layer16_output)\n",
    "layer18_output = MaxPooling1D(pool_size=1)(x)  # This will be layer 18 (sixth branch point)\n",
    "\n",
    "# Final layers (ignore the last Conv1D(128) block)\n",
    "x = Dropout(0.3)(layer18_output)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "main_output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Create the main model (equivalent to your model6)\n",
    "model6_functional = Model(inputs=input_layer, outputs=main_output)\n",
    "model6_functional.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Copy weights from your original model6 to the new functional model\n",
    "for i, layer in enumerate(model6_functional.layers[1:], 1):  # Skip input layer\n",
    "    if i <= len(model6.layers) and model6.layers[i-1].get_weights():  # If layer has weights\n",
    "        try:\n",
    "            layer.set_weights(model6.layers[i-1].get_weights())\n",
    "        except ValueError:\n",
    "            # Skip layers that don't match (Dropout, Flatten, Dense layers with different shapes)\n",
    "            continue\n",
    "\n",
    "print(\"Model6 functional model created and weights copied!\")\n",
    "\n",
    "# Common callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=25)\n",
    "]\n",
    "\n",
    "# Branch Model 1 - from layer 3 equivalent (after first Conv1D block)\n",
    "x1 = Dropout(0.3)(layer1_output)\n",
    "x1 = Flatten()(x1)\n",
    "x1 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x1)\n",
    "x1 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Dropout(0.3)(x1)\n",
    "x1 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x1)\n",
    "x1 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x1)\n",
    "branch_output1 = Dense(1, activation='sigmoid')(x1)\n",
    "model6c1 = Model(inputs=input_layer, outputs=branch_output1)\n",
    "model6c1.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model6c1 created successfully!\")\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Branch Model 2 - from layer 6 equivalent\n",
    "x2 = Dropout(0.3)(layer4_output)\n",
    "x2 = Flatten()(x2)\n",
    "x2 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x2)\n",
    "x2 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Dropout(0.3)(x2)\n",
    "x2 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x2)\n",
    "x2 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x2)\n",
    "branch_output2 = Dense(1, activation='sigmoid')(x2)\n",
    "model6c2 = Model(inputs=input_layer, outputs=branch_output2)\n",
    "model6c2.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model6c2 created successfully!\")\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Branch Model 3 - from layer 9 equivalent\n",
    "x3 = Dropout(0.3)(layer7_output)\n",
    "x3 = Flatten()(x3)\n",
    "x3 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x3)\n",
    "x3 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x3)\n",
    "x3 = BatchNormalization()(x3)\n",
    "x3 = Dropout(0.3)(x3)\n",
    "x3 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x3)\n",
    "x3 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x3)\n",
    "branch_output3 = Dense(1, activation='sigmoid')(x3)\n",
    "model6c3 = Model(inputs=input_layer, outputs=branch_output3)\n",
    "model6c3.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model6c3 created successfully!\")\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Branch Model 4 - from layer 12 equivalent\n",
    "x4 = Dropout(0.3)(layer10_output)\n",
    "x4 = Flatten()(x4)\n",
    "x4 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x4)\n",
    "x4 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x4)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Dropout(0.3)(x4)\n",
    "x4 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x4)\n",
    "x4 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x4)\n",
    "branch_output4 = Dense(1, activation='sigmoid')(x4)\n",
    "model6c4 = Model(inputs=input_layer, outputs=branch_output4)\n",
    "model6c4.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model6c4 created successfully!\")\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Branch Model 5 - from layer 15 equivalent\n",
    "x5 = Dropout(0.3)(layer13_output)\n",
    "x5 = Flatten()(x5)\n",
    "x5 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x5)\n",
    "x5 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x5)\n",
    "x5 = BatchNormalization()(x5)\n",
    "x5 = Dropout(0.3)(x5)\n",
    "x5 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x5)\n",
    "x5 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x5)\n",
    "branch_output5 = Dense(1, activation='sigmoid')(x5)\n",
    "model6c5 = Model(inputs=input_layer, outputs=branch_output5)\n",
    "model6c5.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model6c5 created successfully!\")\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Branch Model 6 - from layer 18 equivalent\n",
    "x6 = Dropout(0.3)(layer16_output)\n",
    "x6 = Flatten()(x6)\n",
    "x6 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x6)\n",
    "x6 = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x6)\n",
    "x6 = BatchNormalization()(x6)\n",
    "x6 = Dropout(0.3)(x6)\n",
    "x6 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x6)\n",
    "x6 = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x6)\n",
    "branch_output6 = Dense(1, activation='sigmoid')(x6)\n",
    "model6c6 = Model(inputs=input_layer, outputs=branch_output6)\n",
    "model6c6.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model6c6 created successfully!\")\n",
    "print(\"All model6 branch models created successfully!\")\n",
    "\n",
    "# Verify the models work\n",
    "print(f\"\\nModel6c1 input shape: {model6c1.input_shape}\")\n",
    "print(f\"Model6c1 output shape: {model6c1.output_shape}\")\n",
    "print(f\"Model6c2 input shape: {model6c2.input_shape}\")\n",
    "print(f\"Model6c2 output shape: {model6c2.output_shape}\")\n",
    "print(f\"Model6c3 input shape: {model6c3.input_shape}\")\n",
    "print(f\"Model6c3 output shape: {model6c3.output_shape}\")\n",
    "print(f\"Model6c4 input shape: {model6c4.input_shape}\")\n",
    "print(f\"Model6c4 output shape: {model6c4.output_shape}\")\n",
    "print(f\"Model6c5 input shape: {model6c5.input_shape}\")\n",
    "print(f\"Model6c5 output shape: {model6c5.output_shape}\")\n",
    "print(f\"Model6c6 input shape: {model6c6.input_shape}\")\n",
    "print(f\"Model6c6 output shape: {model6c6.output_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24703445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9ccb80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e309db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4974450d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57748e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa72f228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fd4e3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models with the correct input shapes for each group\n",
    "models_group_1 = [\n",
    "    model1, model1c1, model1c2, model1c3, model1c4,\n",
    "    model2, model2c1, model2c2, model2c3, model2c4, model2c5, model2c6\n",
    "]\n",
    "\n",
    "models_group_2 = [\n",
    "    model3, model3c1, model3c2, model3c3, model3c4,\n",
    "    model4, model4c1, model4c2, model4c3, model4c4, model4c5, model4c6\n",
    "]\n",
    "\n",
    "models_group_3 = [\n",
    "    model5, model5c1, model5c2, model5c3, model5c4,\n",
    "    model6, model6c1, model6c2, model6c3, model6c4, model6c5, model6c6\n",
    "]\n",
    "\n",
    "# Combine all models\n",
    "models = models_group_1 + models_group_2 + models_group_3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bd6813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4472d92f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c248021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c358251",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 150)\n",
      "(70, 84)\n",
      "(70, 420)\n",
      "(177, 150)\n",
      "(177, 84)\n",
      "(177, 420)\n",
      "(70,)\n",
      "(70,)\n",
      "(70,)\n",
      "(177,)\n",
      "(177,)\n",
      "(177,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_1.shape)\n",
    "print(X_test_2.shape)\n",
    "print(X_test_3.shape)\n",
    "print(X_external_1.shape)\n",
    "print(X_external_2.shape)\n",
    "print(X_external_3.shape)\n",
    "print(y_test_1.shape)\n",
    "print(y_test_2.shape)\n",
    "print(y_test_3.shape)\n",
    "print(y_external_1.shape)\n",
    "print(y_external_2.shape)\n",
    "print(y_external_3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f81f71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39844c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9bef3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377c4941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d25443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf32bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24f88e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc8e8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
